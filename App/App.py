# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.inspection import permutation_importance
import joblib
from io import BytesIO
# =======================
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© (Page Config)
# =======================
st.set_page_config(
    page_title="Student Scores ML Suite",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)
# =======================
# ÙˆØ¸Ø§Ø¦Ù Ù…Ø³Ø§Ø¹Ø¯Ø©
# =======================
TARGETS = ["MathScore", "ReadingScore", "WritingScore"]
@st.cache_data(show_spinner=False)
def load_data(default_path: str = "/mnt/data/Expanded_data_with_more_features.csv"):
    try:
        df = pd.read_csv(default_path)
        return df
    except Exception as e:
        st.warning("ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ Ù…Ù„Ù CSV Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ.")
        return None
def detect_columns(df: pd.DataFrame):
    targets_found = [t for t in TARGETS if t in df.columns]
    feature_cols = [c for c in df.columns if c not in targets_found]
    num_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = [c for c in feature_cols if c not in num_cols]
    return feature_cols, num_cols, cat_cols, targets_found
def metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)   # Ù…Ù† ØºÙŠØ± squared=False
    rmse = mse ** 0.5                          # Ø§Ø­Ø³Ø¨ RMSE ÙŠØ¯ÙˆÙŠ
    r2 = r2_score(y_true, y_pred)
    return {"MAE": mae, "RMSE": rmse, "R2": r2}
def fmt_metric(m):
    return f"{m:.4f}"
def build_pipeline(num_features, cat_features, random_state=42, n_estimators=400):
    pre = ColumnTransformer(
        transformers=[
            ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_features)
        ],
        remainder="passthrough",
        verbose_feature_names_out=False
    )
    model = RandomForestRegressor(
        n_estimators=n_estimators,
        random_state=random_state,
        n_jobs=-1
    )
    pipe = Pipeline([("prep", pre), ("model", model)])
    return pipe
@st.cache_resource(show_spinner=False)
def train_model(df, target, test_size=0.2, random_state=42):
    feature_cols, num_cols, cat_cols, targets_found = detect_columns(df)
    if target not in targets_found:
        raise ValueError(f"Ø§Ù„Ù‡Ø¯Ù {target} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©.")
    X = df[feature_cols].copy()
    y = df[target].copy()
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    # Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨
    pipe = build_pipeline(num_cols, cat_cols, random_state=random_state)
    # ØªØ¯Ø±ÙŠØ¨
    pipe.fit(X_train, y_train)
    # ØªÙ‚ÙŠÙŠÙ…
    y_pred = pipe.predict(X_test)
    mets = metrics(y_test, y_pred)
    # Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø®ØµØ§Ø¦Øµ (Permutation Importance) Ø¹Ù„Ù‰ Ø§Ù„Ø¹ÙŠÙ†Ø©
    try:
        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø© Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø­Ø³Ø§Ø¨
        subsample = min(1000, X_test.shape[0])
        idx = np.random.RandomState(0).choice(X_test.index, size=subsample, replace=False)
        result = permutation_importance(pipe, X_test.loc[idx], y_test.loc[idx], n_repeats=5, random_state=0, n_jobs=-1)
        # Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ±Ù…ÙŠØ²
        feature_names = pipe.named_steps["prep"].get_feature_names_out()
        importances = pd.DataFrame({
            "feature": feature_names,
            "importance_mean": result.importances_mean,
            "importance_std": result.importances_std
        }).sort_values("importance_mean", ascending=False)
    except Exception:
        importances = pd.DataFrame(columns=["feature", "importance_mean", "importance_std"])
    return {
        "pipeline": pipe,
        "metrics": mets,
        "X_columns": X.columns.tolist(),
        "num_cols": num_cols,
        "cat_cols": cat_cols,
        "feature_importances": importances
    }
def download_dataframe(df: pd.DataFrame, filename: str):
    csv = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ CSV", data=csv, file_name=filename, mime="text/csv")
def render_metrics(mets):
    c1, c2, c3 = st.columns(3)
    c1.metric("MAE", fmt_metric(mets["MAE"]))
    c2.metric("RMSE", fmt_metric(mets["RMSE"]))
    c3.metric("RÂ²", fmt_metric(mets["R2"]))
def render_feature_form(reference_df: pd.DataFrame, feature_cols, num_cols, cat_cols, key_prefix="single_"):
    inputs = {}
    for col in feature_cols:
        if col in num_cols:
            # Ù‚ÙŠÙ… Ø§ÙØªØ±Ø§Ø¶ÙŠØ©: Ø§Ù„ÙˆØ³Ø· Ø£Ùˆ 0
            default = float(reference_df[col].dropna().median()) if col in reference_df.columns else 0.0
            val = st.number_input(f"{col}", value=float(default), key=f"{key_prefix}{col}")
        else:
            # Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø© + Ø®ÙŠØ§Ø± ÙƒØªØ§Ø¨Ø© Ù‚ÙŠÙ…Ø© Ø¬Ø¯ÙŠØ¯Ø©
            options = []
            if col in reference_df.columns:
                options = sorted([str(x) for x in reference_df[col].dropna().unique().tolist()][:200])
            choice = st.selectbox(f"{col}", options=options, key=f"{key_prefix}{col}")
            val = choice
        inputs[col] = val
    return pd.DataFrame([inputs])
def header(title, subtitle=None, icon="ğŸ“Š"):
    st.markdown(f"### {icon} {title}")
    if subtitle:
        st.caption(subtitle)
# =======================
# Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ (Sidebar)
# =======================
with st.sidebar:
    st.title("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    st.write("ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ Ù…Ù„Ù CSV Ø¨Ø¯ÙŠÙ„ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ.")
    uploaded_csv = st.file_uploader("Ø±ÙØ¹ Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª (CSV)", type=["csv"], accept_multiple_files=False)
    default_path = "/mnt/data/Expanded_data_with_more_features.csv"
    if uploaded_csv is not None:
        df = pd.read_csv(uploaded_csv)
    else:
        df = load_data(default_path)
    st.divider()
    st.write("Ø§Ø®ØªÙŠØ§Ø±ÙŠ: ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø¯Ø±Ù‘Ø¨ (Ù…Ù„Ù .pkl) Ù„ÙƒÙ„ Ù‡Ø¯Ù")
    up_math = st.file_uploader("MathScore Model (.pkl)", type=["pkl"], key="math_pkl")
    up_read = st.file_uploader("ReadingScore Model (.pkl)", type=["pkl"], key="read_pkl")
    up_write = st.file_uploader("WritingScore Model (.pkl)", type=["pkl"], key="write_pkl")
if df is None or df.empty:
    st.error("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª. Ù…Ù† ÙØ¶Ù„Ùƒ Ø§Ø±ÙØ¹ Ù…Ù„Ù CSV Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ.")
    st.stop()
# Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
feature_cols, num_cols, cat_cols, targets_found = detect_columns(df)
# =======================
# Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# =======================
st.title("Ù…Ù†ØµØ© Ø§Ù„ØªÙˆÙ‚Ø¹ Ù„Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø·Ù„Ø§Ø¨ (Streamlit)")
st.caption("ÙˆØ§Ø¬Ù‡Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø«Ù„Ø§Ø«Ø© Ù†Ù…Ø§Ø°Ø¬: MathScore, ReadingScore, WritingScore â€” ØªØ¯Ø±ÙŠØ¨ØŒ ØªÙ‚ÙŠÙŠÙ…ØŒ ÙˆØªÙ†Ø¨Ø¤ ÙØ±Ø¯ÙŠ/Ø¬Ù…Ø§Ø¹ÙŠ.")
# ØªØ¨ÙˆÙŠØ¨Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©
tab_overview, tab_train, tab_predict, tab_bulk = st.tabs(["Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©", "ØªØ¯Ø±ÙŠØ¨/ØªÙ‚ÙŠÙŠÙ…", "ØªÙ†Ø¨Ø¤ ÙØ±Ø¯ÙŠ", "ØªÙ†Ø¨Ø¤ Ø¬Ù…Ø§Ø¹ÙŠ"])
# -----------------------
# Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©
# -----------------------
with tab_overview:
    header("Ù…Ù„Ø®Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", "Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©", icon="ğŸ§¾")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ", f"{df.shape[0]:,}")
    c2.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©", f"{df.shape[1]:,}")
    c3.metric("Ø£Ø¹Ù…Ø¯Ø© Ø¹Ø¯Ø¯ÙŠØ©", f"{len(num_cols)}")
    c4.metric("Ø£Ø¹Ù…Ø¯Ø© ÙØ¦ÙˆÙŠØ©", f"{len(cat_cols)}")
    with st.expander("Ø¹Ø±Ø¶ 10 ØµÙÙˆÙ"):
        st.dataframe(df.sample(10), use_container_width=True)
    with st.expander("Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©"):
        na = df.isna().sum().reset_index()
        na.columns = ["column", "missing_count"]
        st.dataframe(na, use_container_width=True)
    st.info("Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…ÙƒØªØ´ÙØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: " + (", ".join(targets_found) if targets_found else "â€” Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©."))
# -----------------------
# ØªØ¯Ø±ÙŠØ¨/ØªÙ‚ÙŠÙŠÙ…
# -----------------------
with tab_train:
    st.subheader("ØªØ¯Ø±ÙŠØ¨ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
    st.caption("ÙÙŠ Ø­Ø§Ù„ Ø¹Ø¯Ù… Ø±ÙØ¹ Ù…Ù„ÙØ§Øª .pklØŒ Ø³ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RandomForest Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ© (Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª).")
    target_choice = st.multiselect("Ø§Ø®ØªØ± Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ØªØ¯Ø±ÙŠØ¨Ù‡Ø§/ØªÙ‚ÙŠÙŠÙ…Ù‡Ø§", TARGETS, default=targets_found if targets_found else TARGETS)
    test_size = st.slider("Ø­Ø¬Ù… Ø¹ÙŠÙ†Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (test_size)", 0.1, 0.4, 0.2, step=0.05)
    if st.button("ğŸš€ ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ¯Ø±ÙŠØ¨/Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"):
        res_store = {}
        for tgt in target_choice:
            if st.session_state.get(f"uploaded_{tgt}"):
                # Ø¥Ø°Ø§ ØªÙ… Ø±ÙØ¹ Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø³Ø¨Ù‚Ù‹Ø§ ÙˆØ­ÙÙØ¸ ÙÙŠ Ø§Ù„Ø¬Ù„Ø³Ø©
                res = st.session_state[f"uploaded_{tgt}"]
            elif (tgt == "MathScore" and up_math) or (tgt == "ReadingScore" and up_read) or (tgt == "WritingScore" and up_write):
                # ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Ù…Ù† pkl ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… ÙÙ‚Ø·
                up = up_math if tgt == "MathScore" else up_read if tgt == "ReadingScore" else up_write
                try:
                    pipe = joblib.load(up)
                    X = df[[c for c in feature_cols if c in getattr(pipe, "feature_names_in_", feature_cols)]]
                    y = df[tgt] if tgt in df.columns else None
                    mets = None
                    if y is not None:
                        # ØªÙ‚Ø³ÙŠÙ… Ù„Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø³Ø±ÙŠØ¹
                        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=test_size, random_state=42)
                        pipe.fit(X_tr, y_tr)
                        y_pred = pipe.predict(X_te)
                        mets = metrics(y_te, y_pred)
                    res = {"pipeline": pipe, "metrics": mets, "X_columns": X.columns.tolist(),
                           "num_cols": num_cols, "cat_cols": cat_cols,
                           "feature_importances": pd.DataFrame()}
                    st.session_state[f"uploaded_{tgt}"] = res
                except Exception as e:
                    st.error(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ {tgt}: {e}")
                    continue
            else:
                res = train_model(df, tgt, test_size=test_size)
                st.session_state[f"uploaded_{tgt}"] = res
            res_store[tgt] = res
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        for tgt, res in res_store.items():
            st.markdown("---")
            header(f"Ù†ØªØ§Ø¦Ø¬ {tgt}", "Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ£Ù‡Ù…ÙŠØ© Ø§Ù„Ø®ØµØ§Ø¦Øµ", icon="âœ…")
            if res["metrics"]:
                render_metrics(res["metrics"])
            else:
                st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…ØªØ§Ø­Ø© (ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ø¯ÙˆÙ† Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨).")
            if not res["feature_importances"].empty:
                with st.expander("Ø¹Ø±Ø¶ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø®ØµØ§Ø¦Øµ (Permutation Importance)"):
                    st.dataframe(res["feature_importances"].head(30), use_container_width=True)
            # Ø²Ø± Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
            try:
                buf = BytesIO()
                joblib.dump(res["pipeline"], buf)
                st.download_button(f"ğŸ’¾ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ({tgt})", data=buf.getvalue(),
                                   file_name=f"{tgt}_pipeline.pkl", mime="application/octet-stream")
            except Exception:
                pass
# -----------------------
# ØªÙ†Ø¨Ø¤ ÙØ±Ø¯ÙŠ
# -----------------------
with tab_predict:
    st.subheader("ØªÙ†Ø¨Ø¤ ÙØ±Ø¯ÙŠ")
    st.caption("Ø§Ù…Ù„Ø£ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ø«Ù… Ø§Ø®ØªØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø±Ø§Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆÙ‚Ø¹.")
    st.info("ØªÙØ¨Ù†Ù‰ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ù…Ù† Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠÙ‘Ø© Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù‡Ø¯Ù.")
    feature_cols, num_cols, cat_cols, targets_found = detect_columns(df)
    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø±Ø¬Ø¹ÙŠ Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙˆØ±Ù… (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙŠÙ… Ø§Ù„Ø¯Ø§ØªØ§)
    ref_df = df.copy()
    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù‡Ø¯Ù
    tgt = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ù…Ø±Ø§Ø¯ ØªÙˆÙ‚Ø¹Ù‡", TARGETS, index=0)

    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Pipeline Ù…Ù† Ø§Ù„Ø¬Ù„Ø³Ø© Ø£Ùˆ ØªØ¯Ø±ÙŠØ¨ Ø³Ø±ÙŠØ¹ Ø¥Ø°Ø§ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯
    if not st.session_state.get(f"uploaded_{tgt}"):
        try:
            st.session_state[f"uploaded_{tgt}"] = train_model(df, tgt)
        except Exception as e:
            st.error(f"Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ¬Ù‡ÙŠØ² Ù…ÙˆØ¯ÙŠÙ„ {tgt}: {e}")
            st.stop()
    pipe = st.session_state[f"uploaded_{tgt}"]["pipeline"]

    with st.form("single_predict_form"):
        single_input_df = render_feature_form(ref_df, feature_cols, num_cols, cat_cols, key_prefix=f"single_{tgt}_")
        submitted = st.form_submit_button("ğŸ”® ØªÙˆÙ‚Ø¹")
    if submitted:
        try:
            pred = pipe.predict(single_input_df)[0]
            st.success(f"Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ({tgt}): **{pred:.2f}**")
            st.json(single_input_df.to_dict(orient="records")[0])
        except Exception as e:
            st.error(f"ØªØ¹Ø°Ø± Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
# -----------------------
# ØªÙ†Ø¨Ø¤ Ø¬Ù…Ø§Ø¹ÙŠ
# -----------------------
with tab_bulk:
    st.subheader("ØªÙ†Ø¨Ø¤ Ø¬Ù…Ø§Ø¹ÙŠ (Batch)")
    st.caption("Ø§Ø±ÙØ¹ Ù…Ù„Ù CSV ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ÙØ³ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø®ØµØ§Ø¦Øµ (Ø¨Ø¯ÙˆÙ† Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù‡Ø¯Ù Ø¥Ø°Ø§ Ø±ØºØ¨Øª). Ø§Ø®ØªØ± Ø§Ù„Ù‡Ø¯Ù Ø«Ù… Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ù Ø¨Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª.")
    bulk_file = st.file_uploader("Ø±ÙØ¹ Ù…Ù„Ù CSV Ù„Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠ", type=["csv"], key="bulk_csv")
    tgt_bulk = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ù‡Ø¯Ù", TARGETS, index=0, key="bulk_target")
    if bulk_file is not None:
        bulk_df = pd.read_csv(bulk_file)
        missing = [c for c in feature_cols if c not in bulk_df.columns]
        if missing:
            st.error("Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹: " + ", ".join(missing))
        else:
            # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
            if not st.session_state.get(f"uploaded_{tgt_bulk}"):
                try:
                    st.session_state[f"uploaded_{tgt_bulk}"] = train_model(df, tgt_bulk)
                except Exception as e:
                    st.error(f"Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ¬Ù‡ÙŠØ² Ù…ÙˆØ¯ÙŠÙ„ {tgt_bulk}: {e}")
                    st.stop()
            pipe = st.session_state[f"uploaded_{tgt_bulk}"]["pipeline"]

            if st.button("ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠ"):
                try:
                    preds = pipe.predict(bulk_df[feature_cols])
                    out = bulk_df.copy()
                    out[f"{tgt_bulk}_Pred"] = preds
                    st.success("ØªÙ… Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
                    st.dataframe(out.head(20), use_container_width=True)
                    download_dataframe(out, f"{tgt_bulk}_predictions.csv")
                except Exception as e:
                    st.error(f"ØªØ¹Ø°Ø± Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠ: {e}")
# =======================
# Footer
# =======================
st.markdown("""
---
**Ù…Ù„Ø§Ø­Ø¸Ø§Øª ØªÙ‚Ù†ÙŠØ©**  
- ÙÙŠ Ø­Ø§Ù„ Ù„Ù… ØªØ±ÙØ¹ Ù…Ù„ÙØ§Øª .pklØŒ ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ RandomForest ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ù…Ø¹ OneHotEncoder Ù„Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ÙØ¦ÙˆÙŠØ©.  
- ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¤Ù‚ØªÙ‹Ø§ Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡.  
- ÙŠØ¯Ø¹Ù… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„ÙØ±Ø¯ÙŠ ÙˆØ§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠ Ù…Ø¹ ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬.
""")